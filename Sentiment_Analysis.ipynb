{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZH6TB7sWN4dLGfPB7e00a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/NLP_Boiler_Plate/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SY-zfsEwlH"
      },
      "source": [
        "#Load the libraries\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import spacy\r\n",
        "import re,string,unicodedata\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\r\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "from textblob import TextBlob\r\n",
        "from textblob import Word\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\r\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-26IlME3lF"
      },
      "source": [
        "# Data Cleaning Process\r\n",
        "def preprocess(text):\r\n",
        "  text=text.lower()\r\n",
        "  # remove hyperlinks\r\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  #remove hashtag sign\r\n",
        "  text=re.sub(r\"#\",\"\",text)   \r\n",
        "  #remove mentions\r\n",
        "  text = re.sub(r\"(?:\\@)\\w+\", '', text)\r\n",
        "  #remove non ascii chars\r\n",
        "  text=text.encode(\"ascii\",errors=\"ignore\").decode()\r\n",
        "  #remove some puncts (except . ! ?)\r\n",
        "  text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\r\n",
        "  text=re.sub(r'[!]+','!',text)\r\n",
        "  text=re.sub(r'[?]+','?',text)\r\n",
        "  text=re.sub(r'[.]+','.',text)\r\n",
        "  text=re.sub(r\"'\",\"\",text)\r\n",
        "  text=re.sub(r\"\\(\",\"\",text)\r\n",
        "  text=re.sub(r\"\\)\",\"\",text)    \r\n",
        "  text=\" \".join(text.split())\r\n",
        "  return text\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}