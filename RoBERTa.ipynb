{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RoBERTa.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1Zl-J5p48xDiwbh4mrGozPcuQE9jr4ZZc",
      "authorship_tag": "ABX9TyN4ItcxOwTCHf53VYb7u/ov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/NLP_Boiler_Plate/blob/main/RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITa24her4AgO",
        "outputId": "e8ebc01f-afea-4c04-c86f-e22dc04320d2"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 17.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=dec9032e5b1d8fa29198ae6983bc34362f82f054e5d4d503eba76c1c98e3a749\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9C_1Ir93Ixj"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import warnings\r\n",
        "import random\r\n",
        "import torch \r\n",
        "from torch import nn\r\n",
        "import torch.optim as optim\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "import tokenizers\r\n",
        "from transformers import RobertaModel, RobertaConfig\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AAJURd04Zch"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/training.1600000.processed.noemoticon.csv',header=None,error_bad_lines=False,engine='python')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP2czIka8Iz1"
      },
      "source": [
        "def seed_everything(seed_value):\r\n",
        "    random.seed(seed_value)\r\n",
        "    np.random.seed(seed_value)\r\n",
        "    torch.manual_seed(seed_value)\r\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\r\n",
        "    \r\n",
        "    if torch.cuda.is_available(): \r\n",
        "        torch.cuda.manual_seed(seed_value)\r\n",
        "        torch.cuda.manual_seed_all(seed_value)\r\n",
        "        torch.backends.cudnn.deterministic = True\r\n",
        "        torch.backends.cudnn.benchmark = True\r\n",
        "\r\n",
        "seed = 42\r\n",
        "seed_everything(seed)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1kqRukX5Tdf"
      },
      "source": [
        "df1 = df[[0,5]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCn0bFwL6Lqx"
      },
      "source": [
        "df1.columns = ['sentiment','text']\r\n",
        "df1['text'] = df1['text'].astype(str)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpEkWtyh6kLK"
      },
      "source": [
        "# Splitting the data\r\n",
        "X = df1['text']\r\n",
        "y = df1['sentiment']\r\n",
        "X_train, X_test, y_train, y_test  = train_test_split(X,y, test_size=0.10, random_state=42)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrC1vAWV7UlL"
      },
      "source": [
        "class TweetDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, df, max_len=96):\r\n",
        "        self.df = df\r\n",
        "        self.max_len = max_len\r\n",
        "        self.labeled = 'selected_text' in df\r\n",
        "        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\r\n",
        "            vocab_file='../input/roberta-base/vocab.json', \r\n",
        "            merges_file='../input/roberta-base/merges.txt', \r\n",
        "            lowercase=True,\r\n",
        "            add_prefix_space=True)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        data = {}\r\n",
        "        row = self.df.iloc[index]\r\n",
        "        \r\n",
        "        ids, masks, tweet, offsets = self.get_input_data(row)\r\n",
        "        data['ids'] = ids\r\n",
        "        data['masks'] = masks\r\n",
        "        data['tweet'] = tweet\r\n",
        "        data['offsets'] = offsets\r\n",
        "        \r\n",
        "        if self.labeled:\r\n",
        "            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\r\n",
        "            data['start_idx'] = start_idx\r\n",
        "            data['end_idx'] = end_idx\r\n",
        "        \r\n",
        "        return data\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.df)\r\n",
        "    \r\n",
        "    def get_input_data(self, row):\r\n",
        "        tweet = \" \" + \" \".join(row.text.lower().split())\r\n",
        "        encoding = self.tokenizer.encode(tweet)\r\n",
        "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\r\n",
        "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\r\n",
        "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\r\n",
        "                \r\n",
        "        pad_len = self.max_len - len(ids)\r\n",
        "        if pad_len > 0:\r\n",
        "            ids += [1] * pad_len\r\n",
        "            offsets += [(0, 0)] * pad_len\r\n",
        "        \r\n",
        "        ids = torch.tensor(ids)\r\n",
        "        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\r\n",
        "        offsets = torch.tensor(offsets)\r\n",
        "        \r\n",
        "        return ids, masks, tweet, offsets\r\n",
        "        \r\n",
        "    def get_target_idx(self, row, tweet, offsets):\r\n",
        "        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\r\n",
        "\r\n",
        "        len_st = len(selected_text) - 1\r\n",
        "        idx0 = None\r\n",
        "        idx1 = None\r\n",
        "\r\n",
        "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\r\n",
        "            if \" \" + tweet[ind: ind+len_st] == selected_text:\r\n",
        "                idx0 = ind\r\n",
        "                idx1 = ind + len_st - 1\r\n",
        "                break\r\n",
        "\r\n",
        "        char_targets = [0] * len(tweet)\r\n",
        "        if idx0 != None and idx1 != None:\r\n",
        "            for ct in range(idx0, idx1 + 1):\r\n",
        "                char_targets[ct] = 1\r\n",
        "\r\n",
        "        target_idx = []\r\n",
        "        for j, (offset1, offset2) in enumerate(offsets):\r\n",
        "            if sum(char_targets[offset1: offset2]) > 0:\r\n",
        "                target_idx.append(j)\r\n",
        "\r\n",
        "        start_idx = target_idx[0]\r\n",
        "        end_idx = target_idx[-1]\r\n",
        "        \r\n",
        "        return start_idx, end_idx\r\n",
        "        \r\n",
        "def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\r\n",
        "    train_df = df.iloc[train_idx]\r\n",
        "    val_df = df.iloc[val_idx]\r\n",
        "\r\n",
        "    train_loader = torch.utils.data.DataLoader(\r\n",
        "        TweetDataset(train_df), \r\n",
        "        batch_size=batch_size, \r\n",
        "        shuffle=True, \r\n",
        "        num_workers=2,\r\n",
        "        drop_last=True)\r\n",
        "\r\n",
        "    val_loader = torch.utils.data.DataLoader(\r\n",
        "        TweetDataset(val_df), \r\n",
        "        batch_size=batch_size, \r\n",
        "        shuffle=False, \r\n",
        "        num_workers=2)\r\n",
        "\r\n",
        "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\r\n",
        "\r\n",
        "    return dataloaders_dict\r\n",
        "\r\n",
        "def get_test_loader(df, batch_size=32):\r\n",
        "    loader = torch.utils.data.DataLoader(\r\n",
        "        TweetDataset(df), \r\n",
        "        batch_size=batch_size, \r\n",
        "        shuffle=False, \r\n",
        "        num_workers=2)    \r\n",
        "    return loader"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSFVDcRh7onB"
      },
      "source": [
        "class TweetModel(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(TweetModel, self).__init__()\r\n",
        "        \r\n",
        "        config = RobertaConfig.from_pretrained(\r\n",
        "            '../input/roberta-base/config.json', output_hidden_states=True)    \r\n",
        "        self.roberta = RobertaModel.from_pretrained(\r\n",
        "            '../input/roberta-base/pytorch_model.bin', config=config)\r\n",
        "        self.dropout = nn.Dropout(0.5)\r\n",
        "        self.fc = nn.Linear(config.hidden_size, 2)\r\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\r\n",
        "        nn.init.normal_(self.fc.bias, 0)\r\n",
        "\r\n",
        "    def forward(self, input_ids, attention_mask):\r\n",
        "        _, _, hs = self.roberta(input_ids, attention_mask)\r\n",
        "         \r\n",
        "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\r\n",
        "        x = torch.mean(x, 0)\r\n",
        "        x = self.dropout(x)\r\n",
        "        x = self.fc(x)\r\n",
        "        start_logits, end_logits = x.split(1, dim=-1)\r\n",
        "        start_logits = start_logits.squeeze(-1)\r\n",
        "        end_logits = end_logits.squeeze(-1)\r\n",
        "                \r\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY2rrgRo8U4G"
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\r\n",
        "    ce_loss = nn.CrossEntropyLoss()\r\n",
        "    start_loss = ce_loss(start_logits, start_positions)\r\n",
        "    end_loss = ce_loss(end_logits, end_positions)    \r\n",
        "    total_loss = start_loss + end_loss\r\n",
        "    return total_loss\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8G_RN4D8Z5F"
      },
      "source": [
        "def get_selected_text(text, start_idx, end_idx, offsets):\r\n",
        "    selected_text = \"\"\r\n",
        "    for ix in range(start_idx, end_idx + 1):\r\n",
        "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\r\n",
        "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\r\n",
        "            selected_text += \" \"\r\n",
        "    return selected_text\r\n",
        "\r\n",
        "def jaccard(str1, str2): \r\n",
        "    a = set(str1.lower().split()) \r\n",
        "    b = set(str2.lower().split())\r\n",
        "    c = a.intersection(b)\r\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\r\n",
        "\r\n",
        "def compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\r\n",
        "    start_pred = np.argmax(start_logits)\r\n",
        "    end_pred = np.argmax(end_logits)\r\n",
        "    if start_pred > end_pred:\r\n",
        "        pred = text\r\n",
        "    else:\r\n",
        "        pred = get_selected_text(text, start_pred, end_pred, offsets)\r\n",
        "        \r\n",
        "    true = get_selected_text(text, start_idx, end_idx, offsets)\r\n",
        "    \r\n",
        "    return jaccard(true, pred)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSFt701Y8aCp"
      },
      "source": [
        "def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\r\n",
        "    model.cuda()\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        for phase in ['train', 'val']:\r\n",
        "            if phase == 'train':\r\n",
        "                model.train()\r\n",
        "            else:\r\n",
        "                model.eval()\r\n",
        "\r\n",
        "            epoch_loss = 0.0\r\n",
        "            epoch_jaccard = 0.0\r\n",
        "            \r\n",
        "            for data in (dataloaders_dict[phase]):\r\n",
        "                ids = data['ids'].cuda()\r\n",
        "                masks = data['masks'].cuda()\r\n",
        "                tweet = data['tweet']\r\n",
        "                offsets = data['offsets'].numpy()\r\n",
        "                start_idx = data['start_idx'].cuda()\r\n",
        "                end_idx = data['end_idx'].cuda()\r\n",
        "\r\n",
        "                optimizer.zero_grad()\r\n",
        "\r\n",
        "                with torch.set_grad_enabled(phase == 'train'):\r\n",
        "\r\n",
        "                    start_logits, end_logits = model(ids, masks)\r\n",
        "\r\n",
        "                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\r\n",
        "                    \r\n",
        "                    if phase == 'train':\r\n",
        "                        loss.backward()\r\n",
        "                        optimizer.step()\r\n",
        "\r\n",
        "                    epoch_loss += loss.item() * len(ids)\r\n",
        "                    \r\n",
        "                    start_idx = start_idx.cpu().detach().numpy()\r\n",
        "                    end_idx = end_idx.cpu().detach().numpy()\r\n",
        "                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\r\n",
        "                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\r\n",
        "                    \r\n",
        "                    for i in range(len(ids)):                        \r\n",
        "                        jaccard_score = compute_jaccard_score(\r\n",
        "                            tweet[i],\r\n",
        "                            start_idx[i],\r\n",
        "                            end_idx[i],\r\n",
        "                            start_logits[i], \r\n",
        "                            end_logits[i], \r\n",
        "                            offsets[i])\r\n",
        "                        epoch_jaccard += jaccard_score\r\n",
        "                    \r\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\r\n",
        "            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\r\n",
        "            \r\n",
        "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\r\n",
        "                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\r\n",
        "    \r\n",
        "    torch.save(model.state_dict(), filename)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AXhNXRs8mPM"
      },
      "source": [
        "num_epochs = 3\r\n",
        "batch_size = 32\r\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "r8Smf8iM_Vdx",
        "outputId": "97a1624d-945c-4aee-b4d4-4a14d724765b"
      },
      "source": [
        "df1"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment                                               text\n",
              "0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1                0  is upset that he can't update his Facebook by ...\n",
              "2                0  @Kenichan I dived many times for the ball. Man...\n",
              "3                0    my whole body feels itchy and like its on fire \n",
              "4                0  @nationwideclass no, it's not behaving at all....\n",
              "...            ...                                                ...\n",
              "1599995          4  Just woke up. Having no school is the best fee...\n",
              "1599996          4  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997          4  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998          4  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999          4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "__TTnCtW_fPK",
        "outputId": "4db56b0c-f330-4d96-e072-ea66db894b64"
      },
      "source": [
        "df1"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment                                               text\n",
              "0                0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1                0  is upset that he can't update his Facebook by ...\n",
              "2                0  @Kenichan I dived many times for the ball. Man...\n",
              "3                0    my whole body feels itchy and like its on fire \n",
              "4                0  @nationwideclass no, it's not behaving at all....\n",
              "...            ...                                                ...\n",
              "1599995          4  Just woke up. Having no school is the best fee...\n",
              "1599996          4  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997          4  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998          4  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999          4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "rPBdnbaO-E4n",
        "outputId": "2f0a35f8-9933-4cda-d985-5dcdfc31b955"
      },
      "source": [
        "%%time\r\n",
        "train_df = df1\r\n",
        "train_df['text'] = train_df['text'].astype(str)\r\n",
        "\r\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \r\n",
        "    print(f'Fold: {fold}')\r\n",
        "\r\n",
        "    model = TweetModel()\r\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\r\n",
        "    criterion = loss_fn    \r\n",
        "    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\r\n",
        "\r\n",
        "    train_model(\r\n",
        "        model, \r\n",
        "        dataloaders_dict,\r\n",
        "        criterion, \r\n",
        "        optimizer, \r\n",
        "        num_epochs,\r\n",
        "        f'roberta_fold{fold}.pth')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400 Client Error: Bad Request for url: https://huggingface.co/../input/roberta-base/config.json/resolve/main/config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://huggingface.co/../input/roberta-base/config.json/resolve/main/config.json",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-f617efcaf486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_df = df1\\ntrain_df['text'] = train_df['text'].astype(str)\\n\\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \\n    print(f'Fold: {fold}')\\n\\n    model = TweetModel()\\n    optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\\n    criterion = loss_fn    \\n    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\\n\\n    train_model(\\n        model, \\n        dataloaders_dict,\\n        criterion, \\n        optimizer, \\n        num_epochs,\\n        f'roberta_fold{fold}.pth')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-300ae993121a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         config = RobertaConfig.from_pretrained(\n\u001b[0;32m----> 6\u001b[0;31m             '../input/roberta-base/config.json', output_hidden_states=True)    \n\u001b[0m\u001b[1;32m      7\u001b[0m         self.roberta = RobertaModel.from_pretrained(\n\u001b[1;32m      8\u001b[0m             '../input/roberta-base/pytorch_model.bin', config=config)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \"\"\"\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             )\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load config for '../input/roberta-base/config.json'. Make sure that:\n\n- '../input/roberta-base/config.json' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '../input/roberta-base/config.json' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ]
    }
  ]
}