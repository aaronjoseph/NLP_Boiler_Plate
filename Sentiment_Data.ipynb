{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentiment_Data.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOgDMXdo6M0spxtWbGO0mgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/NLP_Boiler_Plate/blob/main/Sentiment_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzzvv1BjMs2w",
        "outputId": "a4464af2-609a-40f5-d556-fbb9f89bb5a1"
      },
      "source": [
        "! pip install mlflow\r\n",
        "#Load the libraries\r\n",
        "from mlflow import log_metric, log_param, log_artifacts\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import spacy\r\n",
        "import re,string,unicodedata\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "from textblob import TextBlob\r\n",
        "from textblob import Word\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "import collections, numpy\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from lightgbm import LGBMClassifier\r\n",
        "tqdm.pandas()\r\n",
        "from sklearn.model_selection import train_test_split,KFold\r\n",
        "nltk.download('stopwords')\r\n",
        "! pip install neptune-client==0.4.132\r\n",
        "import neptune\r\n",
        "neptune.init(\r\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiOGI1M2QwMmItZGQyMi00MTBmLThhZTktZDY0OThjYmZlMGMyIn0=\",\r\n",
        "    project_qualified_name=\"aaronjosephmathew/sandbox\"\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.19.5)\n",
            "Requirement already satisfied: alembic<=1.4.1 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.12.4)\n",
            "Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.18.1)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: azure-storage-blob>=12.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (12.7.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.8.1)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.14.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.22)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.15.0)\n",
            "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (4.4.1)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.1.12)\n",
            "Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from mlflow) (20.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic<=1.4.1->mlflow) (1.1.4)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic<=1.4.1->mlflow) (1.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.0->mlflow) (51.3.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.6/dist-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\n",
            "Requirement already satisfied: msrest>=0.6.18 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob>=12.0.0->mlflow) (0.6.19)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob>=12.0.0->mlflow) (1.10.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob>=12.0.0->mlflow) (3.3.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.7)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=4.0.0->mlflow) (0.57.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython>=2.1.0->mlflow) (4.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic<=1.4.1->mlflow) (1.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow) (1.3.0)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow) (0.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow) (1.14.4)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow) (3.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow) (2.20)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: neptune-client==0.4.132 in /usr/local/lib/python3.6/dist-packages (0.4.132)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (20.8)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (0.18.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.3.0)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (7.0.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (2.23.0)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (11.0.2)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (3.1.0)\n",
            "Requirement already satisfied: websocket-client>=0.35.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (0.57.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (3.1.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (1.1.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from neptune-client==0.4.132) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->neptune-client==0.4.132) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->neptune-client==0.4.132) (2.10)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.17.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.13)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (2.8.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (5.17.0)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (1.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from bravado->neptune-client==0.4.132) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.0.8->neptune-client==0.4.132) (4.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client==0.4.132) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->neptune-client==0.4.132) (2018.9)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (2.7.3)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (0.2)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (2.6.0)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.4.132) (3.0.4)\n",
            "Requirement already satisfied: rfc3987; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (1.3.8)\n",
            "Requirement already satisfied: webcolors; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (1.11.1)\n",
            "Requirement already satisfied: strict-rfc3339; extra == \"format\" in /usr/local/lib/python3.6/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.4.132) (0.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Project(aaronjosephmathew/sandbox)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhm3yJynN5gk"
      },
      "source": [
        "# \r\n",
        "sentiment_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',header=None,error_bad_lines=False,engine='python')\r\n",
        "#sentiment_data = sentiment_data.sample(n=1_00_000,replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYi6xVPzqOwZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkZzieW5N5m-"
      },
      "source": [
        "# Data Cleaning Process\r\n",
        "def preprocess(text):\r\n",
        "  text=text.lower()\r\n",
        "  # remove hyperlinks\r\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  #remove hashtag sign\r\n",
        "  text=re.sub(r\"#\",\"\",text)   \r\n",
        "  #remove mentions\r\n",
        "  text = re.sub(r\"(?:\\@)\\w+\", '', text)\r\n",
        "  #remove non ascii chars\r\n",
        "  text=text.encode(\"ascii\",errors=\"ignore\").decode()\r\n",
        "  #remove some puncts (except . ! ?)\r\n",
        "  text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\r\n",
        "  text=re.sub(r'[!]+','!',text)\r\n",
        "  text=re.sub(r'[?]+','?',text)\r\n",
        "  text=re.sub(r'[.]+','.',text)\r\n",
        "  text=re.sub(r\"'\",\"\",text)\r\n",
        "  text=re.sub(r\"\\(\",\"\",text)\r\n",
        "  text=re.sub(r\"\\)\",\"\",text)    \r\n",
        "  text=\" \".join(text.split())\r\n",
        "  return text\r\n",
        "\r\n",
        "def simple_stemmer(text):\r\n",
        "  ps=nltk.porter.PorterStemmer()\r\n",
        "  text= ' '.join([ps.stem(word) for word in text.split()])\r\n",
        "  return text\r\n",
        "\r\n",
        "#removing the stopwords\r\n",
        "def remove_stopwords(text, is_lower_case=False):\r\n",
        "  #set stopwords to english\r\n",
        "  stop=set(stopwords.words('english'))\r\n",
        "  # Tokenization\r\n",
        "  tokenizer=ToktokTokenizer()\r\n",
        "  #Setting English stopwords\r\n",
        "  stopword_list=nltk.corpus.stopwords.words('english')\r\n",
        "  tokens = tokenizer.tokenize(text)\r\n",
        "  tokens = [token.strip() for token in tokens]\r\n",
        "  if is_lower_case:\r\n",
        "      filtered_tokens = [token for token in tokens if token not in stopword_list]\r\n",
        "  else:\r\n",
        "      filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\r\n",
        "  filtered_text = ' '.join(filtered_tokens)    \r\n",
        "  return filtered_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8VBmhmzN5rn",
        "outputId": "25020b25-9715-443b-ab65-70dcf3e3598a"
      },
      "source": [
        "#Apply function\r\n",
        "sentiment_data['clean']=sentiment_data[5].progress_apply(preprocess)\r\n",
        "sentiment_data['stem']=sentiment_data['clean'].progress_apply(simple_stemmer)\r\n",
        "sentiment_data['token'] = sentiment_data['stem'].progress_apply(remove_stopwords) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:01<00:00, 54301.99it/s]\n",
            "100%|██████████| 100000/100000 [00:19<00:00, 5053.93it/s]\n",
            "100%|██████████| 100000/100000 [00:38<00:00, 2596.52it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBWjnS0v4Ify"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, auc, roc_curve, accuracy_score\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBtTXjUBN5zz"
      },
      "source": [
        "le = LabelEncoder()\r\n",
        "sentiment_data['output'] = le.fit_transform(sentiment_data[0])\r\n",
        "# Splitting the data - Into train_test\r\n",
        "X = sentiment_data['token']\r\n",
        "y = sentiment_data['output']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT4bvu1o5HZl"
      },
      "source": [
        "# Count Vectorizer for Bag of words\r\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "cv_train_reviews=cv.fit_transform(X_train)\r\n",
        "#transformed test reviews\r\n",
        "cv_test_reviews=cv.transform(X_test)\r\n",
        "# TFIDF\r\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "tv_train_reviews=tv.fit_transform(X_train)\r\n",
        "#transformed test reviews\r\n",
        "tv_test_reviews=tv.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3J5_rSa4d-n"
      },
      "source": [
        "# Develop XGBoost Model [Classifier/Regressor]\r\n",
        "xgb_cv = XGBClassifier(max_depth=6,n_estimators=1000).fit(cv_train_reviews,y_train)\r\n",
        "xgb_tfidf = XGBClassifier(max_depth=6,n_estimators=1000).fit(tv_train_reviews,y_train)\r\n",
        "\r\n",
        "# Prediction\r\n",
        "prediction_cv = xgb_cv.predict(cv_test_reviews)\r\n",
        "prediction_tfidf = xgb_tfidf.predict(tv_test_reviews) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ5SoYmyTIo8"
      },
      "source": [
        "def scoring(y_pred,y_true,experiment_Name=\"Standard\"):\r\n",
        "  \r\n",
        "  # Accuracy \r\n",
        "  a_c = accuracy_score(y_true, y_pred)\r\n",
        "  print('Accuracy Score', a_c)\r\n",
        "\r\n",
        "  # Recall & Precision  \r\n",
        "  p_s = precision_score(y_true, y_pred)\r\n",
        "  r_s = recall_score(y_true, y_pred)\r\n",
        "  print('Precision Score',p_s)\r\n",
        "  print('Recall Score', r_s)\r\n",
        "  \r\n",
        "  # F1 Score\r\n",
        "  f1_s = f1_score(y_true, y_pred)\r\n",
        "  print('F1 Score', f1_s)\r\n",
        "\r\n",
        "  neptune.create_experiment(experiment_Name)\r\n",
        "  neptune.log_metric('accuracy_score', a_c)\r\n",
        "  neptune.log_metric('precision_score', p_s)\r\n",
        "  neptune.log_metric('recall_score', r_s)\r\n",
        "  neptune.log_metric('f1_score', f1_s)\r\n",
        "  neptune.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_IpjP6aSxF_",
        "outputId": "0bd2dc6f-1269-4e92-cad1-de57f3c80f24"
      },
      "source": [
        "print(\"Printing Metrics for XGboost Count Vectorizer\")\r\n",
        "scoring(prediction_cv,y_test,\"XGBoost Count Vectorizer\")\r\n",
        "print(\"=\"*120,\"\\nPrinting Metrics for XGboost TFIDF\")\r\n",
        "scoring(prediction_tfidf,y_test,\"XGBoost TFIDF\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Metrics for XGboost Count Vectorizer\n",
            "Accuracy Score 0.49815\n",
            "Precision Score 1.0\n",
            "Recall Score 0.49815\n",
            "F1 Score 0.6650201915696026\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-5\n",
            "======================================================================================================================== \n",
            "Printing Metrics for XGboost TFIDF\n",
            "Accuracy Score 0.49815\n",
            "Precision Score 1.0\n",
            "Recall Score 0.49815\n",
            "F1 Score 0.6650201915696026\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp7MPLGOlXIk"
      },
      "source": [
        "Parameters\r\n",
        "\r\n",
        "**Default parameters**\r\n",
        "\r\n",
        "max_depth=3,\r\n",
        "learning_rate=0.1, \r\n",
        "n_estimators=100, \r\n",
        "verbosity=1, \r\n",
        "silent=None, \r\n",
        "objective='reg:squarederror', \r\n",
        "booster='gbtree', \r\n",
        "n_jobs=1, \r\n",
        "nthread=None, \r\n",
        "gamma=0, \r\n",
        "min_child_weight=1, \r\n",
        "max_delta_step=0, \r\n",
        "subsample=1, \r\n",
        "colsample_bytree=1, \r\n",
        "colsample_bylevel=1, \r\n",
        "colsample_bynode=1, \r\n",
        "reg_alpha=0, \r\n",
        "reg_lambda=1,\r\n",
        "scale_pos_weight=1, \r\n",
        "base_score=0.5, \r\n",
        "random_state=0, \r\n",
        "seed=None,\r\n",
        "missing=None, \r\n",
        "importance_type='gain'\r\n",
        "\r\n",
        "**Explanation of relevant parameters for this kernel.**\r\n",
        "\r\n",
        "* booster: Select the type of model to run at each iteration\r\n",
        "* gbtree: tree-based models\r\n",
        "* gblinear: linear models\r\n",
        "* nthread: default to maximum number of threads available if not set\r\n",
        "* objective: This defines the loss function to be minimized\r\n",
        "\r\n",
        "**Parameters for controlling speed**\r\n",
        "\r\n",
        "subsample: Denotes the fraction of observations to be randomly samples for each tree\r\n",
        "\r\n",
        "colsample_bytree: Subsample ratio of columns when constructing each tree.\r\n",
        "\r\n",
        "n_estimators: Number of trees to fit.\r\n",
        "Important parameters which control overfiting\r\n",
        "\r\n",
        "learning_rate: Makes the model more robust by shrinking the weights on each step\r\n",
        "\r\n",
        "max_depth: The maximum depth of a tree.\r\n",
        "\r\n",
        "min_child_weight: Defines the minimum sum of weights of all observations \r\n",
        "required in a child.\r\n",
        "\r\n",
        "Tuning the hyper-parameters\r\n",
        "\r\n",
        "GridSearchCV params:\r\n",
        "\r\n",
        "estimator: estimator object\r\n",
        "\r\n",
        "param_grid : dict or list of dictionaries\r\n",
        "\r\n",
        "scoring: A single string or a callable to evaluate the predictions on the test \r\n",
        "set. If None, the estimator’s score method is used.\r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\r\n",
        "n_jobs: Number of jobs to run in parallel. None means. -1 means using all processors.\r\n",
        "\r\n",
        "cv: cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIqqhYMuSxOf"
      },
      "source": [
        "# Hyper-parameter Tuning using Grid Search to get better score\r\n",
        "# Defination for Hyperparameter tuning\r\n",
        "#XGBoost hyper-parameter tuning\r\n",
        "def hyperParameterTuning(X_train, y_train):\r\n",
        "    param_tuning = {\r\n",
        "        'learning_rate': [0.01, 0.1],\r\n",
        "        'max_depth': [3, 5, 7, 10],\r\n",
        "        'min_child_weight': [1, 3, 5],\r\n",
        "        'subsample': [0.5, 0.7],\r\n",
        "        'colsample_bytree': [0.5, 0.7],\r\n",
        "        'n_estimators' : [100, 200, 500],\r\n",
        "        'objective': ['reg:squarederror']\r\n",
        "    }\r\n",
        "    xgb_model = XGBClassifier()\r\n",
        "\r\n",
        "    gsearch = GridSearchCV(estimator = xgb_model,\r\n",
        "                           param_grid = param_tuning,                        \r\n",
        "                           cv = 5,\r\n",
        "                           n_jobs = -1,\r\n",
        "                           verbose = 1)\r\n",
        "    gsearch.fit(X_train,y_train)\r\n",
        "    return gsearch.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E5r0jiopRbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bd059d-aa9b-413d-dfbb-87d37cef06ae"
      },
      "source": [
        "cv_train_reviews"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<80000x514305 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 514305 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SBHD2UGSxVv",
        "outputId": "52fd3867-e999-4e89-ef1d-8af90b243853"
      },
      "source": [
        "# Hyperparameter tuning for Count Vectorizer Model\r\n",
        "# hyperParameterTuning(cv_train_reviews,y_train)\r\n",
        "\"\"\"\r\n",
        "Parameters\r\n",
        "{'colsample_bytree': 0.5,\r\n",
        " 'learning_rate': 0.01,\r\n",
        " 'max_depth': 3,\r\n",
        " 'min_child_weight': 1,\r\n",
        " 'n_estimators': 100,\r\n",
        " 'objective': 'reg:squarederror',\r\n",
        " 'subsample': 0.5}\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  7.7min\n",
            "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed: 31.0min\n",
            "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed: 62.0min\n",
            "[Parallel(n_jobs=-1)]: Done 1170 tasks      | elapsed: 112.9min\n",
            "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 153.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'colsample_bytree': 0.5,\n",
              " 'learning_rate': 0.01,\n",
              " 'max_depth': 3,\n",
              " 'min_child_weight': 1,\n",
              " 'n_estimators': 100,\n",
              " 'objective': 'reg:squarederror',\n",
              " 'subsample': 0.5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkkN5F-o3H69"
      },
      "source": [
        "xgb_model_cv = XGBClassifier(\r\n",
        "        objective = 'reg:squarederror',\r\n",
        "        colsample_bytree = 0.5,\r\n",
        "        learning_rate = 0.01,\r\n",
        "        max_depth = 3,\r\n",
        "        min_child_weight = 1,\r\n",
        "        n_estimators = 100,\r\n",
        "        subsample = 0.5)\r\n",
        "\r\n",
        "xgb_model_cv.fit(cv_train_reviews,y_train, verbose=False)\r\n",
        "\r\n",
        "y_pred_cv = xgb_model_cv.predict(cv_test_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "herSOqjI9wMZ",
        "outputId": "c6a051ff-9d45-4a50-ac51-21e0e4563d2a"
      },
      "source": [
        "print(\"Printing Metrics for XGboost Count Vectorizer\")\r\n",
        "scoring(y_pred_cv,y_test,\"XGBoost Count Vectorizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Metrics for XGboost Count Vectorizer\n",
            "Accuracy Score 0.49845\n",
            "Precision Score 0.9997992572518318\n",
            "Recall Score 0.4982991495747874\n",
            "F1 Score 0.6651086702500584\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-jChsx8ZKUM"
      },
      "source": [
        "# Code for LGBM Classifier\r\n",
        "model_lgb = LGBMClassifier(num_leaves=5,\r\n",
        "                              learning_rate=0.05, n_estimators=720,\r\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\r\n",
        "                              bagging_freq = 5, feature_fraction = 0.2319,\r\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\r\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\r\n",
        "model_lgb.fit(cv_train_reviews,y_train)\r\n",
        "model_lgb_pred = model_lgb.predict(cv_test_reviews)\r\n",
        "print(\"Printing Metrics for LGBM Count Vectorizer\")\r\n",
        "scoring(model_lgb_pred,y_test,\"LGBM Count Vectorizer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fszsZRxETPr"
      },
      "source": [
        "Model Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xM2ctOE0wLv"
      },
      "source": [
        "def get_oof(clf, x_train, y_train, x_test):\r\n",
        "    oof_train = np.zeros((ntrain,))\r\n",
        "    oof_test = np.zeros((ntest,))\r\n",
        "    oof_test_skf = np.empty((NFOLDS, ntest))\r\n",
        "\r\n",
        "    for i, (train_index, test_index) in enumerate(kf):\r\n",
        "        x_tr = x_train[train_index]\r\n",
        "        y_tr = y_train[train_index]\r\n",
        "        x_te = x_train[test_index]\r\n",
        "\r\n",
        "        clf.train(x_tr, y_tr)\r\n",
        "\r\n",
        "        oof_train[test_index] = clf.predict(x_te)\r\n",
        "        oof_test_skf[i, :] = clf.predict(x_test)\r\n",
        "\r\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\r\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAMtvSg1FUve"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\r\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOEZ5Wpp0pG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgh3lIweOBD2",
        "outputId": "b19d5064-df57-4d9a-82e6-929e8599c1d2"
      },
      "source": [
        "# Random Forest parameters\r\n",
        "rf_params = {\r\n",
        "    'n_jobs': -1,\r\n",
        "    'n_estimators': 500,\r\n",
        "     'warm_start': True, \r\n",
        "     #'max_features': 0.2,\r\n",
        "    'max_depth': 6,\r\n",
        "    'min_samples_leaf': 2,\r\n",
        "    'max_features' : 'sqrt',\r\n",
        "    'verbose': 0\r\n",
        "}\r\n",
        "\r\n",
        "r_clf = RandomForestClassifier(**rf_params)\r\n",
        "r_clf.fit(cv_train_reviews,y_train)\r\n",
        "r_pred = r_clf.predict(cv_test_reviews)\r\n",
        "\r\n",
        "print(\"Printing Metrics for Random Forest Count Vectorizer\")\r\n",
        "scoring(r_pred,y_test,\"Random Forest Count Vectorizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Metrics for Random Forest Count Vectorizer\n",
            "Accuracy Score 0.49815\n",
            "Precision Score 1.0\n",
            "Recall Score 0.49815\n",
            "F1 Score 0.6650201915696026\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqrUWHx5QCLw",
        "outputId": "c0485290-bc34-46a7-e423-067b357dabc8"
      },
      "source": [
        "# Extra Trees Parameters\r\n",
        "et_params = {\r\n",
        "    'n_jobs': -1,\r\n",
        "    'n_estimators':500,\r\n",
        "    #'max_features': 0.5,\r\n",
        "    'max_depth': 8,\r\n",
        "    'min_samples_leaf': 2,\r\n",
        "    'verbose': 0\r\n",
        "}\r\n",
        "\r\n",
        "e_trees = ExtraTreesClassifier(**et_params)\r\n",
        "e_trees.fit(cv_train_reviews,y_train)\r\n",
        "e_trees_pred = e_trees.predict(cv_test_reviews)\r\n",
        "\r\n",
        "print(\"Printing Metrics for Extra Trees Count Vectorizer\")\r\n",
        "scoring(e_trees_pred,y_test,\"Extra Trees Count Vectorizer\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Metrics for Extra Trees Count Vectorizer\n",
            "Accuracy Score 0.49815\n",
            "Precision Score 0.49815\n",
            "Recall Score 1.0\n",
            "F1 Score 0.6650201915696026\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRwm4QtKRJx-",
        "outputId": "08560e91-0d11-4278-dfbf-cc6f7159af91"
      },
      "source": [
        "# AdaBoost parameters\r\n",
        "ada_params = {\r\n",
        "    'n_estimators': 500,\r\n",
        "    'learning_rate' : 0.75\r\n",
        "}\r\n",
        "\r\n",
        "# Gradient Boosting parameters\r\n",
        "gb_params = {\r\n",
        "    'n_estimators': 500,\r\n",
        "     #'max_features': 0.2,\r\n",
        "    'max_depth': 5,\r\n",
        "    'min_samples_leaf': 2,\r\n",
        "    'verbose': 0\r\n",
        "}\r\n",
        "\r\n",
        "# Support Vector Classifier parameters \r\n",
        "svc_params = {\r\n",
        "    'kernel' : 'linear',\r\n",
        "    'C' : 0.025\r\n",
        "    }\r\n",
        "\r\n",
        "ada_clf = AdaBoostClassifier(**ada_params)\r\n",
        "ada_clf.fit(cv_train_reviews,y_train)\r\n",
        "ada_clf_pred = ada_clf.predict(cv_test_reviews)\r\n",
        "print(\"Printing Metrics for Ada Boost Count Vectorizer\")\r\n",
        "scoring(ada_clf_pred,y_test,\"Ada Boost Count Vectorizer\")\r\n",
        "\r\n",
        "\r\n",
        "gb_clf = GradientBoostingClassifier(**gb_params)\r\n",
        "gb_clf.fit(cv_train_reviews,y_train)\r\n",
        "gb_clf_pred = gb_clf.predict(cv_test_reviews)\r\n",
        "print(\"Printing Metrics for Gra Boost Count Vectorizer\")\r\n",
        "scoring(ada_clf_pred,y_test,\"Gra Boost Count Vectorizer\")\r\n",
        "\r\n",
        "SVC_clf = SVC(**svc_params)\r\n",
        "SVC_clf.fit(cv_train_reviews,y_train)\r\n",
        "SVC_clf_pred = SVC_clf.predict(cv_test_reviews)\r\n",
        "print(\"Printing Metrics for SVM Count Vectorizer\")\r\n",
        "scoring(SVC_clf_pred,y_test,\"SVM Count Vectorizer\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Metrics for Ada Boost Count Vectorizer\n",
            "Accuracy Score 0.4989\n",
            "Precision Score 0.498523153942428\n",
            "Recall Score 0.9994981431295794\n",
            "F1 Score 0.6652414990981361\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-11\n",
            "Printing Metrics for Gra Boost Count Vectorizer\n",
            "Accuracy Score 0.4989\n",
            "Precision Score 0.498523153942428\n",
            "Recall Score 0.9994981431295794\n",
            "F1 Score 0.6652414990981361\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-12\n",
            "Printing Metrics for SVM Count Vectorizer\n",
            "Accuracy Score 0.50275\n",
            "Precision Score 0.5004520795660036\n",
            "Recall Score 1.0\n",
            "F1 Score 0.6670683940946067\n",
            "https://ui.neptune.ai/aaronjosephmathew/sandbox/e/SAN-13\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}