{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentiment_Data.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPfPY/R5ZHH2N4ZoYCR0sJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronjoseph/NLP_Boiler_Plate/blob/main/Sentiment_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzzvv1BjMs2w",
        "outputId": "8aee13cc-3293-41d3-c1f2-58cfbc0a22ab"
      },
      "source": [
        "! pip install mlflow\r\n",
        "#Load the libraries\r\n",
        "from mlflow import log_metric, log_param, log_artifacts\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.preprocessing import LabelBinarizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import spacy\r\n",
        "import re,string,unicodedata\r\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.svm import SVC\r\n",
        "from textblob import TextBlob\r\n",
        "from textblob import Word\r\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "from xgboost import XGBClassifier\r\n",
        "tqdm.pandas()\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.14.1)\n",
            "Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.18.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.5)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.4.1)\n",
            "Requirement already satisfied: alembic<=1.4.1 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.4.1)\n",
            "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.8.1)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: gunicorn; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from mlflow) (20.0.4)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.22)\n",
            "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow) (2.23.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (7.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.13)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.1.12)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow) (1.3.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow) (0.3)\n",
            "Requirement already satisfied: azure-storage-blob>=12.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow) (12.7.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.7->mlflow) (0.8.7)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.6/dist-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow) (2018.9)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic<=1.4.1->mlflow) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic<=1.4.1->mlflow) (1.1.4)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=4.0.0->mlflow) (0.57.0)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.6/dist-packages (from gunicorn; platform_system != \"Windows\"->mlflow) (51.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython>=2.1.0->mlflow) (4.0.5)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow) (2.11.2)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob>=12.0.0->mlflow) (1.10.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob>=12.0.0->mlflow) (3.3.1)\n",
            "Requirement already satisfied: msrest>=0.6.18 in /usr/local/lib/python3.6/dist-packages (from azure-storage-blob>=12.0.0->mlflow) (0.6.19)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic<=1.4.1->mlflow) (1.1.1)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow) (1.14.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow) (1.3.0)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow) (0.6.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow) (2.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-storage-blob>=12.0.0->mlflow) (3.1.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhm3yJynN5gk",
        "outputId": "1e17d277-0fc8-4a5a-d099-6ed883087289"
      },
      "source": [
        "sentiment_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',header=None,error_bad_lines=False,engine='python')\r\n",
        "sentiment_data = sentiment_data.sample(n=1_00_000,replace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 112675: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkZzieW5N5m-"
      },
      "source": [
        "# Data Cleaning Process\r\n",
        "def preprocess(text):\r\n",
        "  text=text.lower()\r\n",
        "  # remove hyperlinks\r\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\r\n",
        "  #remove hashtag sign\r\n",
        "  text=re.sub(r\"#\",\"\",text)   \r\n",
        "  #remove mentions\r\n",
        "  text = re.sub(r\"(?:\\@)\\w+\", '', text)\r\n",
        "  #remove non ascii chars\r\n",
        "  text=text.encode(\"ascii\",errors=\"ignore\").decode()\r\n",
        "  #remove some puncts (except . ! ?)\r\n",
        "  text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\r\n",
        "  text=re.sub(r'[!]+','!',text)\r\n",
        "  text=re.sub(r'[?]+','?',text)\r\n",
        "  text=re.sub(r'[.]+','.',text)\r\n",
        "  text=re.sub(r\"'\",\"\",text)\r\n",
        "  text=re.sub(r\"\\(\",\"\",text)\r\n",
        "  text=re.sub(r\"\\)\",\"\",text)    \r\n",
        "  text=\" \".join(text.split())\r\n",
        "  return text\r\n",
        "\r\n",
        "def simple_stemmer(text):\r\n",
        "  ps=nltk.porter.PorterStemmer()\r\n",
        "  text= ' '.join([ps.stem(word) for word in text.split()])\r\n",
        "  return text\r\n",
        "\r\n",
        "#removing the stopwords\r\n",
        "def remove_stopwords(text, is_lower_case=False):\r\n",
        "  #set stopwords to english\r\n",
        "  stop=set(stopwords.words('english'))\r\n",
        "  # Tokenization\r\n",
        "  tokenizer=ToktokTokenizer()\r\n",
        "  #Setting English stopwords\r\n",
        "  stopword_list=nltk.corpus.stopwords.words('english')\r\n",
        "  tokens = tokenizer.tokenize(text)\r\n",
        "  tokens = [token.strip() for token in tokens]\r\n",
        "  if is_lower_case:\r\n",
        "      filtered_tokens = [token for token in tokens if token not in stopword_list]\r\n",
        "  else:\r\n",
        "      filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\r\n",
        "  filtered_text = ' '.join(filtered_tokens)    \r\n",
        "  return filtered_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8VBmhmzN5rn",
        "outputId": "93e00e99-8812-4062-b894-8d040d138611"
      },
      "source": [
        "#Apply function\r\n",
        "sentiment_data['clean']=sentiment_data[5].progress_apply(preprocess)\r\n",
        "sentiment_data['stem']=sentiment_data['clean'].progress_apply(simple_stemmer)\r\n",
        "sentiment_data['token'] = sentiment_data['stem'].progress_apply(remove_stopwords) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:01<00:00, 66032.01it/s]\n",
            "100%|██████████| 100000/100000 [00:16<00:00, 6188.40it/s]\n",
            "100%|██████████| 100000/100000 [00:26<00:00, 3800.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBtTXjUBN5zz"
      },
      "source": [
        "# Splitting the data - Into train_test\r\n",
        "X = sentiment_data['token']\r\n",
        "y = sentiment_data[0]\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42) \r\n",
        "# Count Vectorizer for Bag of words\r\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "cv_train_reviews=cv.fit_transform(X_train)\r\n",
        "#transformed test reviews\r\n",
        "cv_test_reviews=cv.transform(X_test)\r\n",
        "# TFIDF\r\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "tv_train_reviews=tv.fit_transform(X_train)\r\n",
        "#transformed test reviews\r\n",
        "tv_test_reviews=tv.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3J5_rSa4d-n"
      },
      "source": [
        "# Develop XGBoost Model [Classifier/Regressor]\r\n",
        "xgb_cv = XGBClassifier(max_depth=6,n_estimators=1000).fit(cv_train_reviews,y_train)\r\n",
        "xgb_tfidf = XGBClassifier(max_depth=6,n_estimators=1000).fit(tv_train_reviews,y_train)\r\n",
        "\r\n",
        "# Prediction\r\n",
        "prediction_cv = xgb_cv.predict(cv_test_reviews)\r\n",
        "prediction_tfidf = xgb_tfidf.predict(tv_test_reviews) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FMBymXxU_V1"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, auc, roc_curve, accuracy_score\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "Label_Val = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ5SoYmyTIo8"
      },
      "source": [
        "def scoring(y_true,y_pred):\r\n",
        "  # Accuracy \r\n",
        "  print('Accuracy Score', accuracy_score(y_true, y_pred))\r\n",
        "\r\n",
        "  # Recall & Precision  \r\n",
        "  print('Precision Score',precision_score(y_true, y_pred,pos_label=Label_Val))\r\n",
        "  print('Recall Score', recall_score(y_true, y_pred,pos_label=Label_Val))\r\n",
        "  \r\n",
        "  # F1 Score\r\n",
        "  f1_score(y_true, y_pred, pos_label=Label_Val)\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_IpjP6aSxF_",
        "outputId": "0e6678f5-4d2a-423d-b75a-db849d4fab31"
      },
      "source": [
        "print(\"Printing Metrics for XGboost Count Vectorizer\")\r\n",
        "scoring(prediction_cv,y_test)\r\n",
        "print(\"=\"*120,\"\\nPrinting Metrics for XGboost TFIDF\")\r\n",
        "scoring(prediction_tfidf,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing Metrics for XGboost Count Vectorizer\n",
            "Accuracy Score 1.0\n",
            "Precision Score 0.0\n",
            "Recall Score 0.0\n",
            "======================================================================================================================== \n",
            "Printing Metrics for XGboost TFIDF\n",
            "Accuracy Score 1.0\n",
            "Precision Score 0.0\n",
            "Recall Score 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp7MPLGOlXIk"
      },
      "source": [
        "Parameters\r\n",
        "\r\n",
        "**Default parameters**\r\n",
        "\r\n",
        "max_depth=3,\r\n",
        "learning_rate=0.1, \r\n",
        "n_estimators=100, \r\n",
        "verbosity=1, \r\n",
        "silent=None, \r\n",
        "objective='reg:squarederror', \r\n",
        "booster='gbtree', \r\n",
        "n_jobs=1, \r\n",
        "nthread=None, \r\n",
        "gamma=0, \r\n",
        "min_child_weight=1, \r\n",
        "max_delta_step=0, \r\n",
        "subsample=1, \r\n",
        "colsample_bytree=1, \r\n",
        "colsample_bylevel=1, \r\n",
        "colsample_bynode=1, \r\n",
        "reg_alpha=0, \r\n",
        "reg_lambda=1,\r\n",
        "scale_pos_weight=1, \r\n",
        "base_score=0.5, \r\n",
        "random_state=0, \r\n",
        "seed=None,\r\n",
        "missing=None, \r\n",
        "importance_type='gain'\r\n",
        "\r\n",
        "**Explanation of relevant parameters for this kernel.**\r\n",
        "\r\n",
        "* booster: Select the type of model to run at each iteration\r\n",
        "* gbtree: tree-based models\r\n",
        "* gblinear: linear models\r\n",
        "* nthread: default to maximum number of threads available if not set\r\n",
        "* objective: This defines the loss function to be minimized\r\n",
        "\r\n",
        "**Parameters for controlling speed**\r\n",
        "\r\n",
        "subsample: Denotes the fraction of observations to be randomly samples for each tree\r\n",
        "\r\n",
        "colsample_bytree: Subsample ratio of columns when constructing each tree.\r\n",
        "\r\n",
        "n_estimators: Number of trees to fit.\r\n",
        "Important parameters which control overfiting\r\n",
        "\r\n",
        "learning_rate: Makes the model more robust by shrinking the weights on each step\r\n",
        "\r\n",
        "max_depth: The maximum depth of a tree.\r\n",
        "\r\n",
        "min_child_weight: Defines the minimum sum of weights of all observations \r\n",
        "required in a child.\r\n",
        "\r\n",
        "Tuning the hyper-parameters\r\n",
        "\r\n",
        "GridSearchCV params:\r\n",
        "\r\n",
        "estimator: estimator object\r\n",
        "\r\n",
        "param_grid : dict or list of dictionaries\r\n",
        "\r\n",
        "scoring: A single string or a callable to evaluate the predictions on the test \r\n",
        "set. If None, the estimator’s score method is used.\r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\r\n",
        "n_jobs: Number of jobs to run in parallel. None means. -1 means using all processors.\r\n",
        "\r\n",
        "cv: cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIqqhYMuSxOf"
      },
      "source": [
        "# Hyper-parameter Tuning using Grid Search to get better score\r\n",
        "# Defination for Hyperparameter tuning\r\n",
        "#XGBoost hyper-parameter tuning\r\n",
        "def hyperParameterTuning(X_train, y_train):\r\n",
        "    param_tuning = {\r\n",
        "        'learning_rate': [0.01, 0.1],\r\n",
        "        'max_depth': [3, 5, 7, 10],\r\n",
        "        'min_child_weight': [1, 3, 5],\r\n",
        "        'subsample': [0.5, 0.7],\r\n",
        "        'colsample_bytree': [0.5, 0.7],\r\n",
        "        'n_estimators' : [100, 200, 500],\r\n",
        "        'objective': ['reg:squarederror']\r\n",
        "    }\r\n",
        "    xgb_model = XGBClassifier()\r\n",
        "\r\n",
        "    gsearch = GridSearchCV(estimator = xgb_model,\r\n",
        "                           param_grid = param_tuning,                        \r\n",
        "                           cv = 5,\r\n",
        "                           n_jobs = -1,\r\n",
        "                           verbose = 1)\r\n",
        "    gsearch.fit(X_train,y_train)\r\n",
        "    return gsearch.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E5r0jiopRbR"
      },
      "source": [
        "cv_train_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SBHD2UGSxVv",
        "outputId": "52fd3867-e999-4e89-ef1d-8af90b243853"
      },
      "source": [
        "# Hyperparameter tuning for Count Vectorizer Model\r\n",
        "# hyperParameterTuning(cv_train_reviews,y_train)\r\n",
        "\"\"\"\r\n",
        "Parameters\r\n",
        "{'colsample_bytree': 0.5,\r\n",
        " 'learning_rate': 0.01,\r\n",
        " 'max_depth': 3,\r\n",
        " 'min_child_weight': 1,\r\n",
        " 'n_estimators': 100,\r\n",
        " 'objective': 'reg:squarederror',\r\n",
        " 'subsample': 0.5}\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  7.7min\n",
            "[Parallel(n_jobs=-1)]: Done 370 tasks      | elapsed: 31.0min\n",
            "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed: 62.0min\n",
            "[Parallel(n_jobs=-1)]: Done 1170 tasks      | elapsed: 112.9min\n",
            "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed: 153.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'colsample_bytree': 0.5,\n",
              " 'learning_rate': 0.01,\n",
              " 'max_depth': 3,\n",
              " 'min_child_weight': 1,\n",
              " 'n_estimators': 100,\n",
              " 'objective': 'reg:squarederror',\n",
              " 'subsample': 0.5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQRjDEoCnHO4"
      },
      "source": [
        "Best Parameter for CV Data \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSCrNNTh0y8k"
      },
      "source": [
        "ntrain = train.shape[0]\r\n",
        "ntest = test.shape[0]\r\n",
        "SEED = 0 # for reproducibility\r\n",
        "NFOLDS = 5 # set folds for out-of-fold prediction\r\n",
        "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\r\n",
        "\r\n",
        "# Class to extend the Sklearn classifier\r\n",
        "class SklearnHelper(object):\r\n",
        "    def __init__(self, clf, seed=0, params=None):\r\n",
        "        params['random_state'] = seed\r\n",
        "        self.clf = clf(**params)\r\n",
        "\r\n",
        "    def train(self, x_train, y_train):\r\n",
        "        self.clf.fit(x_train, y_train)\r\n",
        "\r\n",
        "    def predict(self, x):\r\n",
        "        return self.clf.predict(x)\r\n",
        "    \r\n",
        "    def fit(self,x,y):\r\n",
        "        return self.clf.fit(x,y)\r\n",
        "    \r\n",
        "    def feature_importances(self,x,y):\r\n",
        "        print(self.clf.fit(x,y).feature_importances_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xM2ctOE0wLv"
      },
      "source": [
        "def get_oof(clf, x_train, y_train, x_test):\r\n",
        "    oof_train = np.zeros((ntrain,))\r\n",
        "    oof_test = np.zeros((ntest,))\r\n",
        "    oof_test_skf = np.empty((NFOLDS, ntest))\r\n",
        "\r\n",
        "    for i, (train_index, test_index) in enumerate(kf):\r\n",
        "        x_tr = x_train[train_index]\r\n",
        "        y_tr = y_train[train_index]\r\n",
        "        x_te = x_train[test_index]\r\n",
        "\r\n",
        "        clf.train(x_tr, y_tr)\r\n",
        "\r\n",
        "        oof_train[test_index] = clf.predict(x_te)\r\n",
        "        oof_test_skf[i, :] = clf.predict(x_test)\r\n",
        "\r\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\r\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOEZ5Wpp0pG5"
      },
      "source": [
        "# Put in our parameters for said classifiers\r\n",
        "# Random Forest parameters\r\n",
        "rf_params = {\r\n",
        "    'n_jobs': -1,\r\n",
        "    'n_estimators': 500,\r\n",
        "     'warm_start': True, \r\n",
        "     #'max_features': 0.2,\r\n",
        "    'max_depth': 6,\r\n",
        "    'min_samples_leaf': 2,\r\n",
        "    'max_features' : 'sqrt',\r\n",
        "    'verbose': 0\r\n",
        "}\r\n",
        "\r\n",
        "# Extra Trees Parameters\r\n",
        "et_params = {\r\n",
        "    'n_jobs': -1,\r\n",
        "    'n_estimators':500,\r\n",
        "    #'max_features': 0.5,\r\n",
        "    'max_depth': 8,\r\n",
        "    'min_samples_leaf': 2,\r\n",
        "    'verbose': 0\r\n",
        "}\r\n",
        "\r\n",
        "# AdaBoost parameters\r\n",
        "ada_params = {\r\n",
        "    'n_estimators': 500,\r\n",
        "    'learning_rate' : 0.75\r\n",
        "}\r\n",
        "\r\n",
        "# Gradient Boosting parameters\r\n",
        "gb_params = {\r\n",
        "    'n_estimators': 500,\r\n",
        "     #'max_features': 0.2,\r\n",
        "    'max_depth': 5,\r\n",
        "    'min_samples_leaf': 2,\r\n",
        "    'verbose': 0\r\n",
        "}\r\n",
        "\r\n",
        "# Support Vector Classifier parameters \r\n",
        "svc_params = {\r\n",
        "    'kernel' : 'linear',\r\n",
        "    'C' : 0.025\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVTMJA5wnGBd"
      },
      "source": [
        "# Create 5 objects that represent our 4 models\r\n",
        "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\r\n",
        "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\r\n",
        "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\r\n",
        "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\r\n",
        "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ5JN60tqDHX"
      },
      "source": [
        "# LightGBM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAYndSIBqElO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwY_RASdqEw-"
      },
      "source": [
        "# Few Models "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnDGQ32vqLGl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83jsi28uqLRt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABCzEtdJqLXF"
      },
      "source": [
        "# Stacking "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR5v7KugqMVr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm1luf_IqMaf"
      },
      "source": [
        "# MLFLOW Capture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0v1pswCqN7o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCMYWgSYqN_8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02ham7HVqODk"
      },
      "source": [
        "# Sentiment Models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzAtz71zqRaG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EyDDDTuqRe0"
      },
      "source": [
        "!pip install mlflow --quiet\r\n",
        "!pip install pyngrok --quiet\r\n",
        "\r\n",
        "import mlflow\r\n",
        "\r\n",
        "with mlflow.start_run(run_name=\"MLflow on Colab\"):\r\n",
        "  mlflow.log_metric(\"m1\", 2.0)\r\n",
        "  mlflow.log_param(\"p1\", \"mlflow-colab\")\r\n",
        "\r\n",
        "# run tracking UI in the background\r\n",
        "get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\r\n",
        "\r\n",
        "\r\n",
        "# create remote tunnel using ngrok.com to allow local port access\r\n",
        "# borrowed from https://colab.research.google.com/github/alfozan/MLflow-GBRT-demo/blob/master/MLflow-GBRT-demo.ipynb#scrollTo=4h3bKHMYUIG6\r\n",
        "\r\n",
        "from pyngrok import ngrok\r\n",
        "\r\n",
        "# Terminate open tunnels if exist\r\n",
        "ngrok.kill()\r\n",
        "\r\n",
        "# Setting the authtoken (optional)\r\n",
        "# Get your authtoken from https://dashboard.ngrok.com/auth\r\n",
        "NGROK_AUTH_TOKEN = \"\"\r\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\r\n",
        "\r\n",
        "# Open an HTTPs tunnel on port 5000 for http://localhost:5000\r\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\r\n",
        "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqPuUL0_qRiS"
      },
      "source": [
        "# "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}